{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchmetrics.functional import accuracy, precision\n",
    "import torchmetrics.functional as tf\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "from positional_encodings.torch_encodings import PositionalEncodingPermute2D\n",
    "\n",
    "from data.cifar100 import CIFAR100DataModule\n",
    "from data.cifar10 import CIFAR10DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to whatever folder you wish CIFAR-100 to be downloaded into\n",
    "CIFAR = \"/media/curttigges/project-files/datasets/cifar-100/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class FourierPositionEncoding(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalImageEmbedding(nn.Module):\n",
    "    \"\"\"Reshapes images and concatenates position encoding\n",
    "    \n",
    "    Initializes position encoding,\n",
    "    \n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.p_enc = PositionalEncodingPermute2D(input_channels)\n",
    "        self.conv = nn.Conv1d(input_channels*2, embed_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initial x of shape [BATCH_SIZE x CHANNELS x HEIGHT x WIDTH]\n",
    "        \n",
    "        enc = self.p_enc(x)\n",
    "        # create position encoding of the same shape as x\n",
    "\n",
    "        x = torch.cat([x, enc], dim=1)\n",
    "        # concatenate position encoding along the channel dimension\n",
    "        # shape is now [BATCH_SIZE x COLOR_CHANNELS + POS_ENC_CHANNELS x HEIGHT x WIDTH]\n",
    "\n",
    "        x = x.flatten(2)\n",
    "        # reshape to [BATCH_SIZE x CHANNELS x HEIGHT*WIDTH]\n",
    "\n",
    "        x = self.conv(x)\n",
    "        # shape is now [BATCH_SIZE x EMBED_DIM x HEIGHT*WIDTH]\n",
    "\n",
    "        x = x.permute(2, 0, 1)\n",
    "        # shape is now [HEIGHT*WIDTH x BATCH_SIZE x EMBED_DIM]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverAttention(nn.Module):\n",
    "    \"\"\"Basic decoder block used both for cross-attention and the latent transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, mlp_dim, n_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lnorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=n_heads)\n",
    "\n",
    "        self.lnorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear1 = nn.Linear(embed_dim, mlp_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.linear2 = nn.Linear(mlp_dim, embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, q):\n",
    "        # x will be of shape [PIXELS x BATCH_SIZE x EMBED_DIM]\n",
    "        # q will be of shape [LATENT_DIM x BATCH_SIZE x EMBED_DIM] when this is\n",
    "        # used for cross-attention; otherwise same as x\n",
    "\n",
    "        # attention block\n",
    "        out = self.lnorm1(x)\n",
    "        out, _ = self.attn(query=q, key=x, value=x)\n",
    "        # out will be of shape [LATENT_DIM x BATCH_SIZE x EMBED_DIM] after matmul\n",
    "        # when used for cross-attention; otherwise same as x\n",
    "        \n",
    "        # first residual connection\n",
    "        resid = out + q\n",
    "\n",
    "        # dense block\n",
    "        out = self.lnorm2(resid)\n",
    "        out = self.linear1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.drop(out)\n",
    "\n",
    "        # second residual connection\n",
    "        out = out + resid\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, mlp_dim, n_heads, dropout, n_layers):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.ModuleList([\n",
    "            PerceiverAttention(\n",
    "                embed_dim=embed_dim, \n",
    "                mlp_dim=mlp_dim, \n",
    "                n_heads=n_heads, \n",
    "                dropout=dropout) \n",
    "            for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, l):\n",
    "        \n",
    "        for trnfr in self.transformer:\n",
    "            l = trnfr(l, l)\n",
    "        \n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, attn_mlp_dim, trnfr_mlp_dim, trnfr_heads, dropout, trnfr_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cross_attention = PerceiverAttention(\n",
    "            embed_dim, attn_mlp_dim, n_heads=1, dropout=dropout)\n",
    "\n",
    "        self.latent_transformer = LatentTransformer(\n",
    "            embed_dim, trnfr_mlp_dim, trnfr_heads, dropout, trnfr_layers)\n",
    "\n",
    "    def forward(self, x, l):\n",
    "        l = self.cross_attention(x, l)\n",
    "\n",
    "        l = self.latent_transformer(l)\n",
    "\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embed_dim, latent_dim, batch_size, n_classes):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fc = nn.Linear(self.embed_dim*self.latent_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # latent, batch, embed\n",
    "        L, B, E = x.shape\n",
    "\n",
    "        #x = x.mean(dim=0)\n",
    "        x = x.transpose(0, 1)\n",
    "        #print(x.shape)\n",
    "        x = torch.reshape(x, (B, E*L))\n",
    "        #print(x.shape)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceiver(nn.Module):\n",
    "    def __init__(\n",
    "        self, latent_dim, embed_dim, attn_mlp_dim, trnfr_mlp_dim, trnfr_heads, \n",
    "        dropout, trnfr_layers, n_blocks, n_classes, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize latent array\n",
    "        self.latent = nn.Parameter(\n",
    "            torch.nn.init.trunc_normal_(\n",
    "                torch.zeros((latent_dim, 1, embed_dim)), \n",
    "                mean=0, \n",
    "                std=0.02, \n",
    "                a=-2, \n",
    "                b=2))\n",
    "        # In the paper, a truncated normal distribution was used for initialization, \n",
    "        # so I used this hidden torch function to create it.\n",
    "\n",
    "        # Initialize embedding with position encoding\n",
    "        self.embed = PositionalImageEmbedding(3, embed_dim)\n",
    "\n",
    "        # Initialize arbitrary number of Perceiver blocks\n",
    "        self.perceiver_blocks = nn.ModuleList([\n",
    "            PerceiverBlock(\n",
    "                embed_dim=embed_dim, \n",
    "                attn_mlp_dim=attn_mlp_dim, \n",
    "                trnfr_mlp_dim=trnfr_mlp_dim, \n",
    "                trnfr_heads=trnfr_heads, \n",
    "                dropout = dropout, \n",
    "                trnfr_layers = trnfr_layers)\n",
    "            for b in range(n_blocks)])\n",
    "\n",
    "        # Initialize classification layer\n",
    "        self.classifier = Classifier(embed_dim=embed_dim, latent_dim=latent_dim, batch_size=batch_size, n_classes=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First we expand our latent query matrix to size of batch\n",
    "        batch_size = x.shape[0]\n",
    "        latent = self.latent.expand(-1, batch_size, -1)\n",
    "\n",
    "        # Next, we pass the image through the embedding module to get flattened input\n",
    "        x = self.embed(x) \n",
    "\n",
    "        # Next, we iteratively pass the latent matrix and image embedding through\n",
    "        # perceiver blocks\n",
    "        for pb in self.perceiver_blocks:\n",
    "            latent = pb(x, latent)\n",
    "        #print(latent.shape)\n",
    "    \n",
    "        # Finally, we project the output to the number of target classes\n",
    "        latent = self.classifier(latent)\n",
    "\n",
    "        \n",
    "\n",
    "        return latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverTrainingModule(pl.LightningModule):\n",
    "    '''Classic Perceiver\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Size of embedding output from linear projection layer\n",
    "        hidden_dim (int): Size of MLP head\n",
    "        class_head_dim (int): Size of classification head\n",
    "        num_encoders (int): Number of encoder layers\n",
    "        num_heads (int): Number of self-attention heads\n",
    "        patch_size (int): Size of patches\n",
    "        num_patches (int): Total count of patches (patch sequence size) \n",
    "        dropout (float): Probability of dropout\n",
    "        batch_size (int): Batch size (used for OneCycleLR)\n",
    "        learning_rate (float): Maximum learning rate\n",
    "        weight_decay (float): Optimizer weight decay\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        latent_dim, \n",
    "        embed_dim,\n",
    "        attn_mlp_dim, \n",
    "        trnfr_mlp_dim, \n",
    "        trnfr_heads, \n",
    "        dropout, \n",
    "        trnfr_layers, \n",
    "        n_blocks, \n",
    "        n_classes,\n",
    "        batch_size, \n",
    "        learning_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        # Key parameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Transformer with arbitrary number of encoders, heads, and hidden size\n",
    "        self.model = Perceiver(\n",
    "            latent_dim=latent_dim, \n",
    "            embed_dim=embed_dim,\n",
    "            attn_mlp_dim=attn_mlp_dim, \n",
    "            trnfr_mlp_dim=trnfr_mlp_dim, \n",
    "            trnfr_heads=trnfr_heads, \n",
    "            dropout=dropout, \n",
    "            trnfr_layers=trnfr_layers, \n",
    "            n_blocks=n_blocks, \n",
    "            n_classes=n_classes,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)        \n",
    "        return x\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), \n",
    "            lr=self.hparams.learning_rate\n",
    "        )\n",
    "        \n",
    "        steps_per_epoch = 60000 // self.hparams.batch_size\n",
    "   \n",
    "        lr_scheduler_dict = {\n",
    "            \"scheduler\":OneCycleLR(\n",
    "                optimizer,\n",
    "                self.hparams.learning_rate,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                anneal_strategy='cos'\n",
    "            ),\n",
    "            \"interval\":\"step\",\n",
    "        }\n",
    "        return {\"optimizer\":optimizer, \"lr_scheduler\":lr_scheduler_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    \"latent_dim\":128,\n",
    "    \"embed_dim\":32,\n",
    "    \"attn_mlp_dim\":128, \n",
    "    \"trnfr_mlp_dim\":128, \n",
    "    \"trnfr_heads\":8, \n",
    "    \"dropout\":0.0, \n",
    "    \"trnfr_layers\":24, \n",
    "    \"n_blocks\":4, \n",
    "    \"n_classes\":100,\n",
    "    \"batch_size\":64,\n",
    "    \"learning_rate\":0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "cifar100 = CIFAR100DataModule(\n",
    "    batch_size=model_kwargs[\"batch_size\"], \n",
    "    num_workers=12, \n",
    "    data_dir=CIFAR)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "model = PerceiverTrainingModule(**model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | Perceiver | 1.7 M \n",
      "------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.738     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|▏         | 10/783 [06:18<8:08:08, 37.89s/it, loss=458, v_num=22] val_loss=4.290, val_acc=0.0446]\n",
      "Epoch 5:  37%|███▋      | 290/783 [13:44<23:21,  2.84s/it, loss=4.06, v_num=23, val_loss=4.090, val_acc=0.0776]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/cv-env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=180,\n",
    "    accelerator='gpu', \n",
    "    devices=1,\n",
    "    #logger=wandb_logger, #comment out if not using WandB\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=10)])\n",
    "    \n",
    "trainer.fit(model, datamodule=cifar100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('cv-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86c88f9a3588ea9343d84fb206e74c1b312a3b0d43eff9010fec7c5800ba29d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
