{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchmetrics.functional import accuracy, precision\n",
    "import torchmetrics.functional as tf\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "from positional_encodings.torch_encodings import PositionalEncodingPermute2D\n",
    "\n",
    "from data.cifar100 import CIFAR100DataModule\n",
    "from data.cifar10 import CIFAR10DataModule\n",
    "from data.mnist import MNISTDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to whatever folder you wish CIFAR-100 to be downloaded into\n",
    "CIFAR = \"/media/curttigges/project-files/datasets/cifar-10/\"\n",
    "MNIST = \"/media/curttigges/project-files/datasets/mnist/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalImageEmbedding(nn.Module):\n",
    "    \"\"\"Reshapes images and concatenates position encoding\n",
    "    \n",
    "    Initializes position encoding,\n",
    "    \n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, input_channels, embed_dim, bands=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = self.fourier_features(\n",
    "            shape=input_shape, bands=bands)\n",
    "        self.conv = nn.Conv1d(input_channels + self.ff.shape[0], embed_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initial x of shape [BATCH_SIZE x CHANNELS x HEIGHT x WIDTH]\n",
    "        \n",
    "\n",
    "        # create position encoding of the same shape as x\n",
    "        enc = self.ff.unsqueeze(0).expand(\n",
    "            (x.shape[0],) + self.ff.shape)\n",
    "        enc = enc.type_as(x)\n",
    "        #print(enc.shape)\n",
    "        x = torch.cat([x, enc], dim=1)\n",
    "        # concatenate position encoding along the channel dimension\n",
    "        # shape is now [BATCH_SIZE x COLOR_CHANNELS + POS_ENC_CHANNELS x HEIGHT x WIDTH]\n",
    "\n",
    "        x = x.flatten(2)\n",
    "        # reshape to [BATCH_SIZE x CHANNELS x HEIGHT*WIDTH]\n",
    "\n",
    "        x = self.conv(x)\n",
    "        # shape is now [BATCH_SIZE x EMBED_DIM x HEIGHT*WIDTH]\n",
    "\n",
    "        x = x.permute(2, 0, 1)\n",
    "        # shape is now [HEIGHT*WIDTH x BATCH_SIZE x EMBED_DIM]\n",
    "\n",
    "        return x\n",
    "\n",
    "    #def learnable_pos_embed(self):\n",
    "    #    pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_size))\n",
    "\n",
    "    def fourier_features(self, shape, bands):\n",
    "        # This first \"shape\" refers to the shape of the input data, not the output of this function\n",
    "        dims = len(shape)\n",
    "\n",
    "        # Every tensor we make has shape: (bands, dimension, x, y, etc...)\n",
    "\n",
    "        # Pos is computed for the second tensor dimension\n",
    "        # (aptly named \"dimension\"), with respect to all\n",
    "        # following tensor-dimensions (\"x\", \"y\", \"z\", etc.)\n",
    "        pos = torch.stack(list(torch.meshgrid(\n",
    "            *(torch.linspace(-1.0, 1.0, steps=n) for n in list(shape))\n",
    "        )))\n",
    "        pos = pos.unsqueeze(0).expand((bands,) + pos.shape)\n",
    "\n",
    "        # Band frequencies are computed for the first\n",
    "        # tensor-dimension (aptly named \"bands\") with\n",
    "        # respect to the index in that dimension\n",
    "        band_frequencies = (torch.logspace(\n",
    "            math.log(1.0),\n",
    "            math.log(shape[0]/2),\n",
    "            steps=bands,\n",
    "            base=math.e\n",
    "        )).view((bands,) + tuple(1 for _ in pos.shape[1:])).expand(pos.shape)\n",
    "\n",
    "        # For every single value in the tensor, let's compute:\n",
    "        #             freq[band] * pi * pos[d]\n",
    "\n",
    "        # We can easily do that because every tensor is the\n",
    "        # same shape, and repeated in the dimensions where\n",
    "        # it's not relevant (e.g. \"bands\" dimension for the \"pos\" tensor)\n",
    "        result = (band_frequencies * math.pi * pos).view((dims * bands,) + shape)\n",
    "\n",
    "        # Use both sin & cos for each band, and then add raw position as well\n",
    "        # TODO: raw position\n",
    "        result = torch.cat([\n",
    "            torch.sin(result),\n",
    "            torch.cos(result),\n",
    "        ], dim=0)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverAttention(nn.Module):\n",
    "    \"\"\"Basic decoder block used both for cross-attention and the latent transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, mlp_dim, n_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lnorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.lnormq = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=n_heads)\n",
    "\n",
    "        self.lnorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear1 = nn.Linear(embed_dim, mlp_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.linear2 = nn.Linear(mlp_dim, embed_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, q):\n",
    "        # x will be of shape [PIXELS x BATCH_SIZE x EMBED_DIM]\n",
    "        # q will be of shape [LATENT_DIM x BATCH_SIZE x EMBED_DIM] when this is\n",
    "        # used for cross-attention; otherwise same as x\n",
    "\n",
    "        # attention block\n",
    "        out = self.lnorm1(x)\n",
    "        #q = self.lnormq(q)\n",
    "        out, _ = self.attn(query=q, key=x, value=x)\n",
    "        # out will be of shape [LATENT_DIM x BATCH_SIZE x EMBED_DIM] after matmul\n",
    "        # when used for cross-attention; otherwise same as x\n",
    "        \n",
    "        # first residual connection\n",
    "        resid = out + q\n",
    "\n",
    "        # dense block\n",
    "        out = self.lnorm2(resid)\n",
    "        out = self.linear1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.drop(out)\n",
    "\n",
    "        # second residual connection\n",
    "        out = out + resid\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentTransformer(nn.Module):\n",
    "    \"\"\"Latent transformer module with n_layers count of decoders\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, mlp_dim, n_heads, dropout, n_layers):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.ModuleList([\n",
    "            PerceiverAttention(\n",
    "                embed_dim=embed_dim, \n",
    "                mlp_dim=mlp_dim, \n",
    "                n_heads=n_heads, \n",
    "                dropout=dropout) \n",
    "            for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, l):\n",
    "        \n",
    "        for trnfr in self.transformer:\n",
    "            l = trnfr(l, l)\n",
    "        \n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverIOBlock(nn.Module):\n",
    "    \"\"\"Block consisting of one latent transformer, preceded by an optional cross-attention\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, attn_mlp_dim, trnfr_mlp_dim, trnfr_heads, dropout, trnfr_layers, inner_ca=False):\n",
    "        super().__init__()\n",
    "        self.inner_ca = inner_ca\n",
    "        \n",
    "        # Optional cross-attention. Can be omitted\n",
    "        if self.inner_ca:\n",
    "            self.cross_attention = PerceiverAttention(\n",
    "                embed_dim, attn_mlp_dim, n_heads=1, dropout=dropout)\n",
    "\n",
    "        self.latent_transformer = LatentTransformer(\n",
    "            embed_dim, trnfr_mlp_dim, trnfr_heads, dropout, trnfr_layers)\n",
    "\n",
    "    def forward(self, x, l):\n",
    "        if self.inner_ca:\n",
    "            l = self.cross_attention(x, l)\n",
    "\n",
    "        l = self.latent_transformer(l)\n",
    "\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierIO(nn.Module):\n",
    "    \"\"\"Perceiver IO classification calculation\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, output_dim, output_heads, n_classes, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # learnable label embedding\n",
    "        self.n_classes = n_classes\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.label_emb = nn.Parameter(torch.rand(n_classes, 1, output_dim))\n",
    "        \n",
    "        self.output_attn = PerceiverAttention(\n",
    "            embed_dim, embed_dim * 4, output_heads, dropout=dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(output_dim, output_dim)\n",
    "        self.fc2 = nn.Linear(self.n_classes * self.output_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # latent, batch, embed\n",
    "        L, B, E = x.shape\n",
    "        #print(f\"Latent shape: {x.shape}\")\n",
    "        #print(f\"Output emb shape: {self.label_emb.shape}\")\n",
    "        output_emb = self.label_emb.repeat(1, B, 1)\n",
    "        #print(f\"Output array shape: {output_emb.shape}\")\n",
    "\n",
    "        x = self.output_attn(x, output_emb)\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = torch.reshape(x,(B, self.n_classes * self.output_dim))\n",
    "        #x = x.mean(dim=0)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"Original Perceiver classification calculation\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, latent_dim, batch_size, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc2 = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # latent, batch, embed\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = x.mean(dim=0)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverIO(nn.Module):\n",
    "    \"\"\"Full Perceiver IO model\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, input_shape, latent_dim, embed_dim, output_dim, attn_mlp_dim, trnfr_mlp_dim, trnfr_heads, \n",
    "        dropout, trnfr_layers, n_blocks, n_classes, batch_size, inner_ca=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize latent array\n",
    "        self.latent = nn.Parameter(\n",
    "            torch.nn.init.trunc_normal_(\n",
    "                torch.zeros((latent_dim, 1, embed_dim)), mean=0, std=0.02, a=-2, b=2))\n",
    "\n",
    "        # Initialize embedding with position encoding\n",
    "        self.embed = PositionalImageEmbedding(input_shape, 1, embed_dim)\n",
    "\n",
    "        # Initialize initial block with cross-attention\n",
    "        self.initial_perceiver_block = PerceiverIOBlock(\n",
    "            embed_dim=embed_dim, \n",
    "            attn_mlp_dim=attn_mlp_dim, \n",
    "            trnfr_mlp_dim=trnfr_mlp_dim, \n",
    "            trnfr_heads=trnfr_heads, \n",
    "            dropout = dropout, \n",
    "            trnfr_layers = trnfr_layers,\n",
    "            inner_ca=True)\n",
    "\n",
    "        # Initialize arbitrary number of Perceiver blocks; will be transformer\n",
    "        # blocks unless inner_ca (inner cross-attention) is enabled\n",
    "        self.perceiver_blocks = nn.ModuleList([\n",
    "            PerceiverIOBlock(\n",
    "                embed_dim=embed_dim, \n",
    "                attn_mlp_dim=attn_mlp_dim, \n",
    "                trnfr_mlp_dim=trnfr_mlp_dim, \n",
    "                trnfr_heads=trnfr_heads, \n",
    "                dropout = dropout, \n",
    "                trnfr_layers = trnfr_layers,\n",
    "                inner_ca=inner_ca)\n",
    "            for b in range(n_blocks)])\n",
    "\n",
    "        # PerceiverIO classification layer\n",
    "        self.classifier = ClassifierIO(\n",
    "            embed_dim=embed_dim, output_dim=output_dim, output_heads=8, n_classes=n_classes)\n",
    "\n",
    "        # Original classification layer\n",
    "        #self.classifier = Classifier(embed_dim=embed_dim, latent_dim=latent_dim, batch_size=batch_size, n_classes=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First we expand our latent query matrix to size of batch\n",
    "        batch_size = x.shape[0]\n",
    "        latent = self.latent.expand(-1, batch_size, -1)\n",
    "\n",
    "        # Next, we pass the image through the embedding module to get flattened input\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # \n",
    "        latent = self.initial_perceiver_block(x, latent)\n",
    "\n",
    "        # Next, we iteratively pass the latent matrix and image embedding through\n",
    "        # perceiver blocks\n",
    "        for pb in self.perceiver_blocks:\n",
    "            latent = pb(x, latent)\n",
    "    \n",
    "        # Finally, we project the output to the number of target classes\n",
    "        latent = self.classifier(latent)\n",
    "\n",
    "        return latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverIOTrainingModule(pl.LightningModule):\n",
    "    '''Classic Perceiver\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple of ints): Dimensions of input images\n",
    "        latent_dim (int): Size of latent array\n",
    "        embed_dim (int): Size of embedding output from linear projection layer\n",
    "        attn_mlp_dim (int): Size of MLP\n",
    "        trnfr_mlp_dim (int): Size transformer MLP\n",
    "        trnfr_heads (int): Number of self-attention heads in the latent transformer \n",
    "        dropout (float): dropout for network\n",
    "        trnfr_layers (int): Number of decoders in the transformers\n",
    "        n_blocks (int): Number of Perceiver blocks\n",
    "        n_classes (int): Number of target classes\n",
    "        batch_size (int): Batch size\n",
    "        learning_rate (float): Learning Rate\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape, \n",
    "        latent_dim, \n",
    "        embed_dim,\n",
    "        output_dim,\n",
    "        attn_mlp_dim, \n",
    "        trnfr_mlp_dim, \n",
    "        trnfr_heads, \n",
    "        dropout, \n",
    "        trnfr_layers, \n",
    "        n_blocks, \n",
    "        n_classes,\n",
    "        batch_size, \n",
    "        learning_rate,\n",
    "        inner_ca):\n",
    "        super().__init__()\n",
    "\n",
    "        # Key parameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Transformer with arbitrary number of encoders, heads, and hidden size\n",
    "        \n",
    "        self.model = PerceiverIO(\n",
    "            input_shape=input_shape,\n",
    "            latent_dim=latent_dim, \n",
    "            embed_dim=embed_dim,\n",
    "            output_dim=output_dim,\n",
    "            attn_mlp_dim=attn_mlp_dim, \n",
    "            trnfr_mlp_dim=trnfr_mlp_dim, \n",
    "            trnfr_heads=trnfr_heads, \n",
    "            dropout=dropout, \n",
    "            trnfr_layers=trnfr_layers, \n",
    "            n_blocks=n_blocks, \n",
    "            n_classes=n_classes,\n",
    "            batch_size=batch_size,\n",
    "            inner_ca=inner_ca\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)        \n",
    "        return x\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), \n",
    "            lr=self.hparams.learning_rate\n",
    "        )\n",
    "        gamma = 0.1 ** 0.5\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=3, gamma=gamma, last_epoch=-1, verbose=False)\n",
    "            \n",
    "        return {\"optimizer\":optimizer, \"lr_scheduler\":lr_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    \"input_shape\":(28, 28),\n",
    "    \"latent_dim\":8,\n",
    "    \"embed_dim\":16,\n",
    "    \"output_dim\":16,\n",
    "    \"attn_mlp_dim\":16, \n",
    "    \"trnfr_mlp_dim\":16, \n",
    "    \"trnfr_heads\":8, \n",
    "    \"dropout\":0.1, \n",
    "    \"trnfr_layers\":6, \n",
    "    \"n_blocks\":6, \n",
    "    \"n_classes\":10,\n",
    "    \"batch_size\":64,\n",
    "    \"learning_rate\":0.003,\n",
    "    \"inner_ca\":True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "cifar10 = CIFAR10DataModule(\n",
    "    batch_size=model_kwargs[\"batch_size\"], \n",
    "    num_workers=12, \n",
    "    data_dir=CIFAR)\n",
    "\n",
    "mnist_dm = MNISTDataModule(\n",
    "    download_dir=MNIST,\n",
    "    batch_size=model_kwargs[\"batch_size\"],\n",
    "    num_workers=12\n",
    ")\n",
    "\n",
    "pl.seed_everything(42)\n",
    "model = PerceiverIOTrainingModule(**model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcurt-tigges\u001b[0m (\u001b[33mascendant\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>training/logs/wandb/run-20220817_173506-39ft8zmg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ascendant/perceiver/runs/39ft8zmg\" target=\"_blank\">divine-river-44</a></strong> to <a href=\"https://wandb.ai/ascendant/perceiver\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | PerceiverIO | 85.5 K\n",
      "--------------------------------------\n",
      "85.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "85.5 K    Total params\n",
      "0.342     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 939/939 [23:34<00:00,  1.51s/it, loss=0.338, v_num=8zmg, val_loss=0.400, val_acc=0.875]   \n"
     ]
    }
   ],
   "source": [
    "# Comment out if not using wandb\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"perceiver\", \n",
    "    save_dir=\"training/logs/\",\n",
    "    log_model=True)\n",
    "wandb_logger.watch(model, log=\"all\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=24,\n",
    "    devices=1,\n",
    "    accelerator='gpu',\n",
    "    logger=wandb_logger, #comment out if not using WandB\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=10)])\n",
    "    \n",
    "trainer.fit(model, datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('cv-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86c88f9a3588ea9343d84fb206e74c1b312a3b0d43eff9010fec7c5800ba29d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
