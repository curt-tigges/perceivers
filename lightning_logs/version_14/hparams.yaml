attn_mlp_dim: 128
batch_size: 256
dropout: 0.0
embed_dim: 32
latent_dim: 128
learning_rate: 0.01
n_blocks: 4
n_classes: 100
trnfr_heads: 8
trnfr_layers: 48
trnfr_mlp_dim: 128
